{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196cb1c8-72e3-4651-9a7a-5cd8b70bcdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy exported\n",
      "Using CUDA: 5 GPUs available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/users/mohammadibrahim-st/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3083c54857944148a887bc7e11be7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/users/mohammadibrahim-st/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 4 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 532577 and validation size 59176\n",
      "Train size 532577 and validation size 59176\n",
      "Training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/users/mohammadibrahim-st/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/100000: Avg Running Loss = 10.518048286437988\n",
      "Step 100/100000: Avg Running Loss = 8.054579963684082\n",
      "Step 200/100000: Avg Running Loss = 7.862500190734863\n",
      "Batch skipped as captions too long.\n",
      "Step 300/100000: Avg Running Loss = 7.869241786003113\n",
      "Step 400/100000: Avg Running Loss = 7.66180025100708\n",
      "Step 500/100000: Avg Running Loss = 7.390713806152344\n",
      "Step 600/100000: Avg Running Loss = 7.151144347190857\n",
      "Step 700/100000: Avg Running Loss = 7.149825778007507\n",
      "Step 800/100000: Avg Running Loss = 7.110813059806824\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from model_network import CLIPPhi2Model, train_model\n",
    "from dataset import collate_fn, llavadataset\n",
    "os.environ['HTTP_PROXY'] = 'http://185.46.212.90:80'\n",
    "os.environ['HTTPS_PROXY'] = 'http://185.46.212.90:80'\n",
    "# Proxy setup, if necessary\n",
    "try:\n",
    "    os.environ['HTTP_PROXY'] = 'http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY'] = 'http://185.46.212.90:80'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']= '0, 1, 2'\n",
    "    print(\"Proxy exported\")\n",
    "except Exception as e:\n",
    "    print(\"Could not set proxy:\", e)\n",
    "\n",
    "# Ensure CUDA is available, otherwise fall back to CPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using CUDA: {torch.cuda.device_count()} GPUs available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"coco_dataset_pickle\", \"rb\") as fp:\n",
    "    coco_unpickle = pickle.load(fp)\n",
    "\n",
    "# Tokenizer and model setup\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name = \"microsoft/phi-2\"\n",
    "train_batch_size = 4\n",
    "val_batch_size = 4\n",
    "tokenizer = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True, use_cache=True)\n",
    "tokenizer.save_pretrained(\"saved_tokenizer\")\n",
    "\n",
    "# Model initialization and DataParallel wrapping\n",
    "MModalGPT = CLIPPhi2Model()\n",
    "if torch.cuda.is_available():\n",
    "    MModalGPT = torch.nn.DataParallel(MModalGPT).to(device)\n",
    "\n",
    "# Data loaders setup\n",
    "train_dataloader = DataLoader(\n",
    "    llavadataset(coco_unpickle, phi_model_name, clip_model_name, 'train', tokenizer),\n",
    "    collate_fn=collate_fn, batch_size=train_batch_size, num_workers=20, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    llavadataset(coco_unpickle, phi_model_name, clip_model_name, 'val', tokenizer),\n",
    "    collate_fn=collate_fn, batch_size=val_batch_size, num_workers=20, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-6)\n",
    "\n",
    "# Set float32_matmul_precision to 'medium'\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Train the model\n",
    "train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps=100000, model_save_step=1000, model_val_step=1000, log_step=100, max_token_filter=35, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835dc1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is for running in local ###\n",
    "import os\n",
    "try:\n",
    "    os.environ['HTTP_PROXY']='http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY']='http://185.46.212.90:80'\n",
    "    print (\"proxy_exported\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from step1_network import CLIPPhi2Model, train_model\n",
    "from step1_dataset import collate_fn, llavadataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability and fallback to CPU if not available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8103b76-5435-4f19-8d1b-bee880404519",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"coco_dataset_pickle\", \"rb\") as fp:   # Unpickling\n",
    "    coco_unpickle = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3980de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b01a16-1989-460e-bce4-be20b1cefd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_name  = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name   = \"microsoft/phi-2\"\n",
    "train_batch_size = 2 #2\n",
    "val_batch_size   = 4 #4\n",
    "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"saved_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "MModalGPT        = CLIPPhi2Model().to(device)\n",
    "max_steps        = 100 #100000\n",
    "model_save_step  = 10 #1000\n",
    "model_val_step   = 2 #1000\n",
    "log_step         = 2 #1000\n",
    "max_token_filter = 35 #35 # memory management restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056e4a3-a835-4dd9-8bb7-cb84e462ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "train_dataloader = DataLoader(llavadataset(coco_unpickle[0:100], phi_model_name,clip_model_name,'train',tokenizer),\n",
    "                  collate_fn=collate_fn, batch_size=train_batch_size, num_workers = 2, shuffle=True, pin_memory=True)\n",
    "val_dataloader   = DataLoader(llavadataset(coco_unpickle[0:100], phi_model_name,clip_model_name,'val',tokenizer),\n",
    "                  collate_fn=collate_fn, batch_size=val_batch_size, num_workers = 2, shuffle=True, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1db0d-7e18-46c7-9a70-748ad26cc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f16870-145b-4752-ad0e-576152d256bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from model_network import CLIPPhi2Model, train_model\n",
    "from dataset import collate_fn, llavadataset\n",
    "os.environ['HTTP_PROXY'] = 'http://185.46.212.90:80'\n",
    "os.environ['HTTPS_PROXY'] = 'http://185.46.212.90:80'\n",
    "# Proxy setup, if necessary\n",
    "try:\n",
    "    os.environ['HTTP_PROXY'] = 'http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY'] = 'http://185.46.212.90:80'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']= '0, 1, 2, 4'\n",
    "    print(\"Proxy exported\")\n",
    "except Exception as e:\n",
    "    print(\"Could not set proxy:\", e)\n",
    "\n",
    "# Ensure CUDA is available, otherwise fall back to CPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using CUDA: {torch.cuda.device_count()} GPUs available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"coco_dataset_pickle\", \"rb\") as fp:\n",
    "    coco_unpickle = pickle.load(fp)\n",
    "\n",
    "# Tokenizer and model setup\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name = \"microsoft/phi-2\"\n",
    "train_batch_size = 4\n",
    "val_batch_size = 4\n",
    "tokenizer = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True, use_cache=True)\n",
    "tokenizer.save_pretrained(\"saved_tokenizer\")\n",
    "\n",
    "# Model initialization and DataParallel wrapping\n",
    "MModalGPT = CLIPPhi2Model()\n",
    "if torch.cuda.is_available():\n",
    "    MModalGPT = torch.nn.DataParallel(MModalGPT).to(device)\n",
    "\n",
    "# Data loaders setup\n",
    "train_dataloader = DataLoader(\n",
    "    llavadataset(coco_unpickle, phi_model_name, clip_model_name, 'train', tokenizer),\n",
    "    collate_fn=collate_fn, batch_size=train_batch_size, num_workers=20, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    llavadataset(coco_unpickle, phi_model_name, clip_model_name, 'val', tokenizer),\n",
    "    collate_fn=collate_fn, batch_size=val_batch_size, num_workers=20, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-5)\n",
    "\n",
    "# Set float32_matmul_precision to 'medium'\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Train the model\n",
    "train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps=100000, model_save_step=500, model_val_step=500, log_step=100, max_token_filter=35, tokenizer=tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
